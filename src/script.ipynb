{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_adj(sentence, keyword, skip_before_keyword=False, skip_after_keyword=False, prev_bag_of_word=5, after_bag_of_word=5) :    \n",
    "    st = StanfordPOSTagger('./english-bidirectional-distsim.tagger', './stanford-postagger-3.9.2.jar')\n",
    "    \n",
    "#     stopword elemination\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n",
    "    punctuation = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'] # remove it if you need punctuation \n",
    "    \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "#     filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [w for w in word_tokens if not w in punctuation] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "#         if w not in stop_words:\n",
    "        if w not in punctuation:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    list_word_tag = st.tag([(\" \").join(filtered_sentence)])\n",
    "    found_keyword = not skip_before_keyword\n",
    "    list_sentiment_word = []\n",
    "    found_adjective = 0\n",
    "    keyword_idx = -1\n",
    "    for idx in range (len(list_word_tag)) :\n",
    "        if (list_word_tag[idx][0] == keyword) :\n",
    "            keyword_idx = idx\n",
    "            break\n",
    "    \n",
    "    if (skip_before_keyword) :\n",
    "        prev_bag_of_word = 0\n",
    "    \n",
    "    if (skip_after_keyword) :\n",
    "        after_bag_of_word = 0\n",
    "        \n",
    "    for idx in range (len(list_word_tag)) :\n",
    "        if (idx > keyword_idx + after_bag_of_word) :\n",
    "                break\n",
    "                \n",
    "        if ((idx >= keyword_idx - prev_bag_of_word) and list_word_tag[idx][1] == 'JJ') :\n",
    "            found_adjective += 1\n",
    "            sentiment_word = list_word_tag[idx][0]\n",
    "            for adverb_idx in range (idx-1, -1, -1) :\n",
    "                if (list_word_tag[adverb_idx][1] == 'RB') :\n",
    "                    sentiment_word = list_word_tag[adverb_idx][0] + \" \" + sentiment_word\n",
    "                else :\n",
    "                    break\n",
    "            list_sentiment_word.append(sentiment_word)\n",
    "            \n",
    "    return (list_sentiment_word, found_adjective)\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(document, skip_before_keyword=False, skip_after_keyword=False, prev_bag_of_word=5, after_bag_of_word=5) :\n",
    "    act = ['acting','role playing','act',' actress','actor','role','portray','character','villain','performance', 'play', 'perform', 'doing']\n",
    "    plot = ['plot','story','storyline','tale','romance','dialog','script','storyteller',' ending','storytelling','revenge','betrayal','writing']\n",
    "    graphic = ['movie',' film',' picture',' moving picture','',' motion picture',' show',' picture show',' pic',' flick',' romantic comedy', 'graphic', 'effect', 'cinematography', 'cinematographi']\n",
    "    \n",
    "    act_sentence = []\n",
    "    plot_sentence = []\n",
    "    graphic_sentence = []\n",
    "    \n",
    "    tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    list_sentence = tokenizer.sentences_from_text(document)\n",
    "    for sentence in list_sentence :\n",
    "        is_sentence_act = False\n",
    "        is_sentence_plot = False\n",
    "        is_sentence_graphic = False\n",
    "        list_word = tokenize.word_tokenize(sentence)\n",
    "        for word in list_word :\n",
    "            if (ps.stem(word.lower()) in act):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    act_sentence.append(adj_word)\n",
    "            if (ps.stem(word.lower()) in plot):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    plot_sentence.append(adj_word)\n",
    "            if (ps.stem(word.lower()) in graphic):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    graphic_sentence.append(adj_word)\n",
    "    return ([act_sentence,plot_sentence, graphic_sentence])\n",
    "\n",
    "def list_to_sentence(list_of_word) :\n",
    "    return((\" \").join(list_of_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTOH PENGGUNAAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6     -1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     0\n",
       "11     1\n",
       "12     1\n",
       "13     1\n",
       "14     0\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18    -1\n",
       "19     1\n",
       "20     1\n",
       "21     1\n",
       "22     0\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     1\n",
       "30     1\n",
       "31     0\n",
       "32     1\n",
       "33     1\n",
       "34     0\n",
       "35     0\n",
       "36     1\n",
       "37     0\n",
       "38     1\n",
       "39     1\n",
       "40     0\n",
       "41     0\n",
       "42     1\n",
       "43     1\n",
       "44     1\n",
       "45     0\n",
       "46     1\n",
       "47     1\n",
       "48     1\n",
       "49    -1\n",
       "Name: label, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0     -1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7     -1\n",
       "8      1\n",
       "9      1\n",
       "10     0\n",
       "11     0\n",
       "12    -1\n",
       "13     1\n",
       "14     0\n",
       "15     1\n",
       "16     0\n",
       "17     1\n",
       "18     1\n",
       "19     1\n",
       "20     0\n",
       "21    -1\n",
       "22     0\n",
       "23     1\n",
       "24    -1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     0\n",
       "29    -1\n",
       "30     0\n",
       "31     0\n",
       "32    -1\n",
       "33     0\n",
       "34     0\n",
       "35    -1\n",
       "36    -1\n",
       "37     1\n",
       "38    -1\n",
       "39     1\n",
       "40     1\n",
       "41     1\n",
       "42     1\n",
       "43     1\n",
       "44     0\n",
       "45    -1\n",
       "46     0\n",
       "47     0\n",
       "48     1\n",
       "49     0\n",
       "Name: label, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     1\n",
       "3     0\n",
       "4     1\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    1\n",
       "13    1\n",
       "14    0\n",
       "15    1\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    1\n",
       "31    0\n",
       "32    0\n",
       "33    0\n",
       "34    0\n",
       "35    0\n",
       "36    0\n",
       "37    0\n",
       "38    0\n",
       "39    0\n",
       "40    1\n",
       "41    0\n",
       "42    0\n",
       "43    0\n",
       "44    0\n",
       "45    0\n",
       "46    0\n",
       "47    0\n",
       "48    0\n",
       "49    0\n",
       "Name: label, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316.94719076156616\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "#your code here    \n",
    "\n",
    "# input data\n",
    "data = pd.read_excel(\"../input/datasetnlp.xlsx\")\n",
    "data = data\n",
    "data.drop(columns=['title'], inplace=True)\n",
    "for column_name in ['acting', 'plot', 'graphic'] :\n",
    "    data.loc[data[column_name] == 'positive', column_name] = 1\n",
    "    data.loc[data[column_name] == 'neutral', column_name] = 0\n",
    "    data.loc[data[column_name] == 'negative', column_name] = -1\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "data.text = data.text.apply(lambda x: preprocessing(x, prev_bag_of_word=1, after_bag_of_word=10))\n",
    "\n",
    "data_act = pd.DataFrame([])\n",
    "data_act['text'] = [i[0] for i in data.iloc[:,0].tolist()]\n",
    "data_act.text = data_act.text.apply(lambda x: list_to_sentence(x))\n",
    "data_act['label'] = data.iloc[:,1]\n",
    "\n",
    "data_plot = pd.DataFrame([])\n",
    "data_plot['text'] = [i[1] for i in data.iloc[:,0].tolist()]\n",
    "data_plot.text = data_plot.text.apply(lambda x: list_to_sentence(x))\n",
    "data_plot['label'] = data.iloc[:,2]\n",
    "\n",
    "data_graphic = pd.DataFrame([])\n",
    "data_graphic['text'] = [i[2] for i in data.iloc[:,0].tolist()]\n",
    "data_graphic.text = data_graphic.text.apply(lambda x: list_to_sentence(x))\n",
    "data_graphic['label'] = data.iloc[:,3]\n",
    "\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_act_counts = count_vect.fit_transform(data_act.text)\n",
    "X_act_counts.shape\n",
    "\n",
    "X_plot_counts = count_vect.fit_transform(data_plot.text)\n",
    "X_plot_counts.shape\n",
    "\n",
    "X_graphic_counts = count_vect.fit_transform(data_graphic.text)\n",
    "X_graphic_counts.shape\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_act_tfidf = tfidf_transformer.fit_transform(X_act_counts)\n",
    "X_act_tfidf.shape\n",
    "\n",
    "X_plot_tfidf = tfidf_transformer.fit_transform(X_plot_counts)\n",
    "X_plot_tfidf.shape\n",
    "\n",
    "X_graphic_tfidf = tfidf_transformer.fit_transform(X_graphic_counts)\n",
    "X_graphic_tfidf.shape\n",
    "\n",
    "# learning\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf1 = MultinomialNB().fit(X_act_tfidf, data_act.label.tolist())\n",
    "clf2 = MultinomialNB().fit(X_plot_tfidf, data_plot.label.tolist())\n",
    "clf3 = MultinomialNB().fit(X_graphic_tfidf, data_graphic.label.tolist())\n",
    "\n",
    "pred1 = clf1.predict(X_act_tfidf)\n",
    "pred2 = clf2.predict(X_plot_tfidf)\n",
    "pred3 = clf3.predict(X_graphic_tfidf)\n",
    "\n",
    "# scoring\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "display(adjusted_mutual_info_score(pred1, data_act.label))\n",
    "display(adjusted_mutual_info_score(pred2, data_plot.label))\n",
    "display(adjusted_mutual_info_score(pred3, data_graphic.label))\n",
    "\n",
    "print (time.time() - start)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
