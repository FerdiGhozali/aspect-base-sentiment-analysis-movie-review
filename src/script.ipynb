{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.5719}\n",
    "# sid.polarity_scores('sad')\n",
    "# {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4767}\n",
    "# sid.polarity_scores('sad man')\n",
    "# {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n",
    "# sid.polarity_scores('not so happy')\n",
    "# {'neg': 0.616, 'neu': 0.384, 'pos': 0.0, 'compound': -0.4964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_adj(sentence, keyword, skip_before_keyword=False, skip_after_keyword=False, prev_bag_of_word=5, after_bag_of_word=5) :    \n",
    "    st = StanfordPOSTagger('./english-bidirectional-distsim.tagger', './stanford-postagger-3.9.2.jar')\n",
    "    \n",
    "#     stopword elemination\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n",
    "    punctuation = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'] # remove it if you need punctuation \n",
    "    \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "#     filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [w for w in word_tokens if not w in punctuation] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "#         if w not in stop_words:\n",
    "        if w not in punctuation:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    list_word_tag = st.tag([(\" \").join(filtered_sentence)])\n",
    "    found_keyword = not skip_before_keyword\n",
    "    list_sentiment_word = []\n",
    "    found_adjective = 0\n",
    "    keyword_idx = -1\n",
    "    for idx in range (len(list_word_tag)) :\n",
    "        if (list_word_tag[idx][0] == keyword) :\n",
    "            keyword_idx = idx\n",
    "            break\n",
    "    \n",
    "    if (skip_before_keyword) :\n",
    "        prev_bag_of_word = 0\n",
    "    \n",
    "    if (skip_after_keyword) :\n",
    "        after_bag_of_word = 0\n",
    "        \n",
    "    for idx in range (len(list_word_tag)) :\n",
    "        if (idx > keyword_idx + after_bag_of_word) :\n",
    "                break\n",
    "                \n",
    "        if ((idx >= keyword_idx - prev_bag_of_word) and list_word_tag[idx][1] == 'JJ') :\n",
    "            found_adjective += 1\n",
    "            sentiment_word = list_word_tag[idx][0]\n",
    "            for adverb_idx in range (idx-1, -1, -1) :\n",
    "                if (list_word_tag[adverb_idx][1] == 'RB') :\n",
    "                    sentiment_word = list_word_tag[adverb_idx][0] + \" \" + sentiment_word\n",
    "                else :\n",
    "                    break\n",
    "            list_sentiment_word.append(sentiment_word)\n",
    "            \n",
    "    return (list_sentiment_word, found_adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document, skip_before_keyword=False, skip_after_keyword=False, prev_bag_of_word=5, after_bag_of_word=5) :\n",
    "    act = ['acting','role playing','act',' actress','actor','role','portray','character','villain','performance', 'play', 'perform', 'doing']\n",
    "    plot = ['plot','story','storyline','tale','romance','dialog','script','storyteller',' ending','storytelling','revenge','betrayal','writing']\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    act_sentence = []\n",
    "    plot_sentence = []\n",
    "    act_score = 0\n",
    "    plot_score = 0\n",
    "    tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    list_sentence = tokenizer.sentences_from_text(document)\n",
    "    for sentence in list_sentence :\n",
    "        is_sentence_act = False\n",
    "        is_sentence_plot = False\n",
    "        list_word = tokenize.word_tokenize(sentence)\n",
    "        for word in list_word :\n",
    "            if (ps.stem(word.lower()) in act):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    act_sentence.append(adj_word)\n",
    "                    act_score += sid.polarity_scores(adj_word)['pos'] - sid.polarity_scores(adj_word)['neg']\n",
    "            if (ps.stem(word.lower()) in plot):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    plot_sentence.append(adj_word)\n",
    "                    plot_score +=  sid.polarity_scores(adj_word)['pos'] - sid.polarity_scores(adj_word)['neg']\n",
    "#     return ([act_sentence,plot_sentence])\n",
    "    return ([act_score,plot_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_sentence(list_of_word) :\n",
    "    return((\" \").join(list_of_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTOH PENGGUNAAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "#your code here    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = pd.read_excel(\"../input/datasetnlp_nographic_no_neutral.xlsx\")\n",
    "data = data\n",
    "data.drop(columns=['title'], inplace=True)\n",
    "for column_name in ['acting', 'plot'] :\n",
    "    data.loc[data[column_name] == 'positive', column_name] = 1\n",
    "    data.loc[data[column_name] == 'negative', column_name] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "data.text = data.text.apply(lambda x: preprocessing(x, prev_bag_of_word=1, after_bag_of_word=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_act = pd.DataFrame([])\n",
    "# data_act['text'] = [i[0] for i in data.iloc[:,0].tolist()]\n",
    "# data_act.text = data_act.text.apply(lambda x: list_to_sentence(x))\n",
    "data_act['sentiment_score'] = [i[0] for i in data.iloc[:,0].tolist()]\n",
    "data_act['label'] = data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = pd.DataFrame([])\n",
    "# data_plot['text'] = [i[1] for i in data.iloc[:,0].tolist()]\n",
    "# data_plot.text = data_plot.text.apply(lambda x: list_to_sentence(x))\n",
    "data_plot['sentiment_score'] = [i[1] for i in data.iloc[:,0].tolist()]\n",
    "data_plot['label'] = data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# count_vect = CountVectorizer()\n",
    "# X_act_counts = count_vect.fit_transform(data_act.text)\n",
    "# X_act_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_plot_counts = count_vect.fit_transform(data_plot.text)\n",
    "# X_plot_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_act_tfidf = tfidf_transformer.fit_transform(X_act_counts)\n",
    "# X_act_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_plot_tfidf = tfidf_transformer.fit_transform(X_plot_counts)\n",
    "# X_plot_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_act = X_act_tfidf\n",
    "# feature_plot = X_plot_tfidf\n",
    "feature_act = pd.DataFrame(data_act['sentiment_score'])\n",
    "feature_plot = pd.DataFrame(data_plot['sentiment_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # learning\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# clf1 = MultinomialNB().fit(X_act_tfidf, data_act.label.tolist())\n",
    "# clf2 = MultinomialNB().fit(X_plot_tfidf, data_plot.label.tolist())\n",
    "# clf3 = MultinomialNB().fit(X_graphic_tfidf, data_graphic.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1 = clf1.predict(X_act_tfidf)\n",
    "# pred2 = clf2.predict(X_plot_tfidf)\n",
    "# pred3 = clf3.predict(X_graphic_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scoring\n",
    "# from sklearn.metrics import adjusted_mutual_info_score\n",
    "# display(adjusted_mutual_info_score(pred1, data_act.label))\n",
    "# display(adjusted_mutual_info_score(pred2, data_plot.label))\n",
    "# display(adjusted_mutual_info_score(pred3, data_graphic.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm1 = svm.SVC().fit(feature_act, data_act.label.tolist())\n",
    "svm2 = svm.SVC().fit(feature_plot, data_plot.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm1 = svm1.predict(pd.DataFrame(feature_act))\n",
    "pred_svm2 = svm2.predict(pd.DataFrame(feature_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf1 = RandomForestClassifier(n_estimators=10)\n",
    "rf1.fit(feature_act, data_act.label.tolist())\n",
    "rf2 = RandomForestClassifier(n_estimators=10)\n",
    "rf2.fit(feature_plot, data_plot.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "pred_rf1 = rf1.predict(feature_act)\n",
    "pred_rf2 = rf2.predict(feature_plot)\n",
    "cv10_pred_rf1 = cross_val_predict(rf1, feature_act, data_act.label.tolist(), cv=10)\n",
    "cv10_pred_rf2 = cross_val_predict(rf2, feature_plot, data_plot.label.tolist(), cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1.score(feature_act, data_act.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2.score(feature_plot, data_plot.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9593495934959351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7999999999999999"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f1_score(cv10_pred_rf1, data_act.label.tolist()),\n",
    "        f1_score(cv10_pred_rf2, data_plot.label.tolist())\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(max_depth=5,objective='binary:logistic',learning_rate=0.001,min_child_weight=1,scale_pos_weight=1)\n",
    "xgb2 = XGBClassifier(max_depth=5,objective='binary:logistic',learning_rate=0.001,min_child_weight=1,scale_pos_weight=1)\n",
    "\n",
    "predict_xgb_1 = cross_val_predict(xgb1, feature_act, data_act.label.tolist(), cv=10)\n",
    "predict_xgb_2 = cross_val_predict(xgb2, feature_plot, data_plot.label.tolist(), cv=10)\n",
    "\n",
    "cv10_predict_xgb_1 = xgb1.fit(feature_act, data_act.label.tolist()).predict(feature_act)\n",
    "cv10_predict_xgb_2 = xgb2.fit(feature_plot, data_plot.label.tolist()).predict(feature_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967741935483871"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7924528301886793"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.967741935483871"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f1_score(predict_xgb_1, data_act.label.tolist()),\n",
    "        f1_score(predict_xgb_2, data_plot.label.tolist()),\n",
    "        f1_score(cv10_predict_xgb_1, data_act.label.tolist()),\n",
    "        f1_score(cv10_predict_xgb_2, data_plot.label.tolist())\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919.7448952198029\n"
     ]
    }
   ],
   "source": [
    "print (time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "a = list(swn.senti_synsets('not'))\n",
    "pos = 0\n",
    "neg = 0\n",
    "pos=pos+a[0].pos_score()\n",
    "neg=neg+a[0].neg_score()\n",
    "display(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
