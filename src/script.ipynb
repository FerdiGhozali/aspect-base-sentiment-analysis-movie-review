{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_adj(sentence, keyword, skip_before_keyword=False, skip_after_keyword=False, prev_bag_of_word=5, after_bag_of_word=5) :    \n",
    "    st = StanfordPOSTagger('./english-bidirectional-distsim.tagger', './stanford-postagger-3.9.2.jar')\n",
    "    \n",
    "#     stopword elemination\n",
    "#     stop_words = set(stopwords.words('english')) \n",
    "#     stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n",
    "    punctuation = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'] # remove it if you need punctuation \n",
    "    \n",
    "    word_tokens = word_tokenize(sentence) \n",
    "#     filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [w for w in word_tokens if not w in punctuation] \n",
    "    filtered_sentence = [] \n",
    "  \n",
    "    for w in word_tokens: \n",
    "#         if w not in stop_words:\n",
    "        if w not in punctuation:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    list_word_tag = st.tag([(\" \").join(filtered_sentence)])\n",
    "    found_keyword = not skip_before_keyword\n",
    "    list_sentiment_word = []\n",
    "    found_adjective = 0\n",
    "    keyword_idx = -1\n",
    "    for idx in range (len(list_word_tag)) :\n",
    "        if (list_word_tag[idx][0] == keyword) :\n",
    "            keyword_idx = idx\n",
    "            break\n",
    "    \n",
    "    if (skip_before_keyword) :\n",
    "        prev_bag_of_word = 0\n",
    "    \n",
    "    if (skip_after_keyword) :\n",
    "        after_bag_of_word = 0\n",
    "        \n",
    "    for idx in range (len(list_word_tag)) :\n",
    "        if (idx > keyword_idx + after_bag_of_word) :\n",
    "                break\n",
    "                \n",
    "        if ((idx >= keyword_idx - prev_bag_of_word) and list_word_tag[idx][1] == 'JJ') :\n",
    "            found_adjective += 1\n",
    "            sentiment_word = list_word_tag[idx][0]\n",
    "            for adverb_idx in range (idx-1, -1, -1) :\n",
    "                if (list_word_tag[adverb_idx][1] == 'RB') :\n",
    "                    sentiment_word = list_word_tag[adverb_idx][0] + \" \" + sentiment_word\n",
    "                else :\n",
    "                    break\n",
    "            list_sentiment_word.append(sentiment_word)\n",
    "            \n",
    "    return (list_sentiment_word, found_adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document, skip_before_keyword=False, skip_after_keyword=False, prev_bag_of_word=5, after_bag_of_word=5) :\n",
    "    act = ['acting','role playing','act',' actress','actor','role','portray','character','villain','performance', 'play', 'perform', 'doing']\n",
    "    plot = ['plot','story','storyline','tale','romance','dialog','script','storyteller',' ending','storytelling','revenge','betrayal','writing']\n",
    "    \n",
    "    act_sentence = []\n",
    "    plot_sentence = []\n",
    "    \n",
    "    tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    list_sentence = tokenizer.sentences_from_text(document)\n",
    "    for sentence in list_sentence :\n",
    "        is_sentence_act = False\n",
    "        is_sentence_plot = False\n",
    "        list_word = tokenize.word_tokenize(sentence)\n",
    "        for word in list_word :\n",
    "            if (ps.stem(word.lower()) in act):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    act_sentence.append(adj_word)\n",
    "            if (ps.stem(word.lower()) in plot):\n",
    "                list_adj_sentence, adj_found = search_adj(sentence, word, prev_bag_of_word=prev_bag_of_word, after_bag_of_word=after_bag_of_word)\n",
    "                for adj_word in (list_adj_sentence) :\n",
    "                    plot_sentence.append(adj_word)\n",
    "    return ([act_sentence,plot_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_sentence(list_of_word) :\n",
    "    return((\" \").join(list_of_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTOH PENGGUNAAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "#your code here    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = pd.read_excel(\"../input/datasetnlp_nographic_no_neutral.xlsx\")\n",
    "data = data\n",
    "data.drop(columns=['title'], inplace=True)\n",
    "for column_name in ['acting', 'plot'] :\n",
    "    data.loc[data[column_name] == 'positive', column_name] = 1\n",
    "    data.loc[data[column_name] == 'negative', column_name] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "tokenizer = tokenize.PunktSentenceTokenizer()\n",
    "data.text = data.text.apply(lambda x: preprocessing(x, prev_bag_of_word=1, after_bag_of_word=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_act = pd.DataFrame([])\n",
    "data_act['text'] = [i[0] for i in data.iloc[:,0].tolist()]\n",
    "data_act.text = data_act.text.apply(lambda x: list_to_sentence(x))\n",
    "data_act['label'] = data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = pd.DataFrame([])\n",
    "data_plot['text'] = [i[1] for i in data.iloc[:,0].tolist()]\n",
    "data_plot.text = data_plot.text.apply(lambda x: list_to_sentence(x))\n",
    "data_plot['label'] = data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 121)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature engineering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_act_counts = count_vect.fit_transform(data_act.text)\n",
    "X_act_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_plot_counts = count_vect.fit_transform(data_plot.text)\n",
    "X_plot_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 121)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_act_tfidf = tfidf_transformer.fit_transform(X_act_counts)\n",
    "X_act_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 45)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_plot_tfidf = tfidf_transformer.fit_transform(X_plot_counts)\n",
    "X_plot_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # learning\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# clf1 = MultinomialNB().fit(X_act_tfidf, data_act.label.tolist())\n",
    "# clf2 = MultinomialNB().fit(X_plot_tfidf, data_plot.label.tolist())\n",
    "# clf3 = MultinomialNB().fit(X_graphic_tfidf, data_graphic.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1 = clf1.predict(X_act_tfidf)\n",
    "# pred2 = clf2.predict(X_plot_tfidf)\n",
    "# pred3 = clf3.predict(X_graphic_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scoring\n",
    "# from sklearn.metrics import adjusted_mutual_info_score\n",
    "# display(adjusted_mutual_info_score(pred1, data_act.label))\n",
    "# display(adjusted_mutual_info_score(pred2, data_plot.label))\n",
    "# display(adjusted_mutual_info_score(pred3, data_graphic.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm1 = svm.SVC().fit(X_act_tfidf, data_act.label.tolist())\n",
    "svm2 = svm.SVC().fit(X_plot_tfidf, data_plot.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_svm1 = svm1.predict(X_act_tfidf)\n",
    "pred_svm2 = svm2.predict(X_plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/cluster/supervised.py:732: FutureWarning: The behavior of AMI will change in version 0.22. To match the behavior of 'v_measure_score', AMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-4.3132773826476156e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "display(adjusted_mutual_info_score(pred_svm1, data_act.label))\n",
    "display(adjusted_mutual_info_score(pred_svm2, data_plot.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf1 = RandomForestClassifier(n_estimators=10)\n",
    "rf1.fit(X_act_tfidf, data_act.label.tolist())\n",
    "rf2 = RandomForestClassifier(n_estimators=10)\n",
    "rf2.fit(X_plot_tfidf, data_plot.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "pred_rf1 = rf1.predict(X_act_tfidf)\n",
    "pred_rf2 = rf2.predict(X_plot_tfidf)\n",
    "cv10_pred_rf1 = cross_val_predict(rf1, X_act_tfidf, data_act.label.tolist(), cv=10)\n",
    "cv10_pred_rf2 = cross_val_predict(rf2, X_plot_tfidf, data_plot.label.tolist(), cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984375"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1.score(X_act_tfidf, data_act.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2.score(X_plot_tfidf, data_plot.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9586776859504132"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7961165048543689"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f1_score(cv10_pred_rf1, data_act.label.tolist()),\n",
    "        f1_score(cv10_pred_rf2, data_plot.label.tolist())\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb1 = XGBClassifier(max_depth=5,objective='binary:logistic',learning_rate=0.001,min_child_weight=1,scale_pos_weight=1)\n",
    "xgb2 = XGBClassifier(max_depth=5,objective='binary:logistic',learning_rate=0.001,min_child_weight=1,scale_pos_weight=1)\n",
    "\n",
    "predict_xgb_1 = cross_val_predict(xgb1, X_act_tfidf, data_act.label.tolist(), cv=10)\n",
    "predict_xgb_2 = cross_val_predict(xgb2, X_plot_tfidf, data_plot.label.tolist(), cv=10)\n",
    "\n",
    "cv10_predict_xgb_1 = xgb1.fit(X_act_tfidf, data_act.label.tolist()).predict(X_act_tfidf)\n",
    "cv10_predict_xgb_2 = xgb2.fit(X_plot_tfidf, data_plot.label.tolist()).predict(X_plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967741935483871"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7924528301886793"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7924528301886793"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f1_score(predict_xgb_1, data_act.label.tolist()),\n",
    "        f1_score(predict_xgb_2, data_plot.label.tolist()),\n",
    "        f1_score(cv10_predict_xgb_1, data_act.label.tolist()),\n",
    "        f1_score(cv10_predict_xgb_2, data_plot.label.tolist())\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576.2254271507263\n"
     ]
    }
   ],
   "source": [
    "print (time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
